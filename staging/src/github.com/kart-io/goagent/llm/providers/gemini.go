package providers

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"cloud.google.com/go/vertexai/genai"
	"google.golang.org/api/iterator"
	"google.golang.org/api/option"

	agentErrors "github.com/kart-io/goagent/errors"
	"github.com/kart-io/goagent/interfaces"
	agentllm "github.com/kart-io/goagent/llm"
	"github.com/kart-io/goagent/llm/common"
	"github.com/kart-io/goagent/llm/constants"
	"github.com/kart-io/goagent/utils/json"
)

// GeminiProvider implements LLM interface for Google Gemini
type GeminiProvider struct {
	*common.BaseProvider
	*common.ProviderCapabilities
	client    *genai.Client
	model     *genai.GenerativeModel
	modelName string
}

// NewGeminiWithOptions creates a new Gemini provider using options pattern
func NewGeminiWithOptions(opts ...agentllm.ClientOption) (*GeminiProvider, error) {
	// 创建 common.BaseProvider，统一处理 Options
	base := common.NewBaseProvider(opts...)

	// 应用 Provider 特定的默认值
	base.ApplyProviderDefaults(
		constants.ProviderGemini,
		"", // Gemini 不使用 BaseURL
		"gemini-pro",
		constants.EnvGeminiBaseURL,
		constants.EnvGeminiModel,
	)

	// 统一处理 API Key
	if err := base.EnsureAPIKey(constants.EnvGeminiAPIKey, constants.ProviderGemini); err != nil {
		return nil, err
	}

	ctx := context.Background()

	// Create client with API key
	client, err := genai.NewClient(ctx, base.Config.APIKey, "", option.WithAPIKey(base.Config.APIKey))
	if err != nil {
		return nil, agentErrors.NewErrorWithCause(agentErrors.CodeAgentConfig, "failed to create gemini client", err).
			WithComponent("gemini").
			WithOperation("new_client")
	}

	modelName := base.Config.Model

	// Initialize the model
	model := client.GenerativeModel(modelName)

	// Configure model parameters using common.BaseProvider methods
	maxTokens := base.GetMaxTokens(0)
	if maxTokens > 0x7FFFFFFF { // Max int32
		maxTokens = 0x7FFFFFFF
	}
	maxTokensInt32 := int32(maxTokens)
	model.MaxOutputTokens = &maxTokensInt32

	temperature := base.GetTemperature(0)
	tempFloat32 := float32(temperature)
	model.Temperature = &tempFloat32

	provider := &GeminiProvider{
		BaseProvider: base,
		ProviderCapabilities: common.NewProviderCapabilities(
			agentllm.CapabilityCompletion,
			agentllm.CapabilityChat,
			agentllm.CapabilityStreaming,
			agentllm.CapabilityToolCalling,
			agentllm.CapabilityEmbedding,
		),
		client:    client,
		model:     model,
		modelName: modelName,
	}

	return provider, nil
}

// Complete implements basic text completion
func (p *GeminiProvider) Complete(ctx context.Context, req *agentllm.CompletionRequest) (*agentllm.CompletionResponse, error) {
	// Create a new chat session
	cs := p.model.StartChat()

	// Convert messages to Gemini format
	// Handle SystemPrompt from config
	if p.Config.SystemPrompt != "" {
		// Gemini doesn't have a dedicated system role in ChatSession history in the same way,
		// but we can prepend it to the first message or rely on model-level instructions if available.
		// For simplicity and compatibility, we'll prepend it to the first user message or history.
		// However, since we are building history from scratch here:
		cs.History = append(cs.History, &genai.Content{
			Parts: []genai.Part{
				genai.Text(p.Config.SystemPrompt),
			},
			Role: constants.RoleUser, // Use user role for system prompt as fallback/standard practice in some Gemini contexts
		})
		cs.History = append(cs.History, &genai.Content{
			Parts: []genai.Part{
				genai.Text("Understood."),
			},
			Role: "model",
		})
	}

	for _, msg := range req.Messages {
		var role string
		switch msg.Role {
		case constants.RoleSystem:
			// Gemini doesn't have a system role, so we'll prepend it to the first user message
			continue
		case constants.RoleUser:
			role = constants.RoleUser
		case constants.RoleAssistant:
			role = "model"
		default:
			role = constants.RoleUser
		}

		cs.History = append(cs.History, &genai.Content{
			Parts: []genai.Part{
				genai.Text(msg.Content),
			},
			Role: role,
		})
	}

	// Get the last message as the prompt
	if len(req.Messages) == 0 {
		return nil, agentErrors.NewError(agentErrors.CodeInvalidInput, "no messages provided").
			WithComponent("gemini").
			WithOperation("complete")
	}

	lastMessage := req.Messages[len(req.Messages)-1]
	if lastMessage.Role != "user" {
		return nil, agentErrors.NewError(agentErrors.CodeInvalidInput, "last message must be from user").
			WithComponent("gemini").
			WithOperation("complete")
	}

	// Apply request-specific parameters using common.BaseProvider
	maxTokens := p.GetMaxTokens(req.MaxTokens)
	if maxTokens > 0x7FFFFFFF { // Max int32
		maxTokens = 0x7FFFFFFF
	}
	maxTokensInt32 := int32(maxTokens)
	p.model.MaxOutputTokens = &maxTokensInt32

	temperature := p.GetTemperature(req.Temperature)
	tempFloat32 := float32(temperature)
	p.model.Temperature = &tempFloat32

	// Send the message
	resp, err := cs.SendMessage(ctx, genai.Text(lastMessage.Content))
	if err != nil {
		return nil, agentErrors.NewErrorWithCause(agentErrors.CodeExternalService, "gemini API call failed", err).
			WithComponent("gemini").
			WithOperation("complete").
			WithContext("model", p.modelName)
	}

	// Extract content from response
	var content strings.Builder
	for _, part := range resp.Candidates[0].Content.Parts {
		if text, ok := part.(genai.Text); ok {
			content.WriteString(string(text))
		}
	}

	return &agentllm.CompletionResponse{
		Content:      content.String(),
		Model:        p.modelName,
		TokensUsed:   int(resp.UsageMetadata.TotalTokenCount),
		FinishReason: string(resp.Candidates[0].FinishReason),
		Provider:     string(constants.ProviderGemini),
		Usage: &interfaces.TokenUsage{
			PromptTokens:     int(resp.UsageMetadata.PromptTokenCount),
			CompletionTokens: int(resp.UsageMetadata.CandidatesTokenCount),
			TotalTokens:      int(resp.UsageMetadata.TotalTokenCount),
		},
	}, nil
}

// Chat implements chat conversation
func (p *GeminiProvider) Chat(ctx context.Context, messages []agentllm.Message) (*agentllm.CompletionResponse, error) {
	return p.Complete(ctx, &agentllm.CompletionRequest{
		Messages: messages,
	})
}

// Stream implements streaming generation
func (p *GeminiProvider) Stream(ctx context.Context, prompt string) (<-chan string, error) {
	tokens := make(chan string, 100)

	// Start a new chat session
	cs := p.model.StartChat()

	go func() {
		defer close(tokens)

		// Send system prompt if configured
		if p.Config.SystemPrompt != "" {
			// We can't easily inject history into a new chat session started from model.StartChat()
			// without manually setting History.
			// But here we are just sending a message.
			// Strategy: Prepend system prompt to the prompt.
			prompt = fmt.Sprintf("System: %s\n\nUser: %s", p.Config.SystemPrompt, prompt)
		}

		iter := cs.SendMessageStream(ctx, genai.Text(prompt))
		for {
			resp, err := iter.Next()
			if errors.Is(err, iterator.Done) {
				break
			}
			if err != nil {
				// Log error but continue
				fmt.Printf("Gemini stream error: %v\n", err)
				break
			}

			// Extract text from response
			for _, part := range resp.Candidates[0].Content.Parts {
				if text, ok := part.(genai.Text); ok {
					select {
					case tokens <- string(text):
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}
				}
			}
		}
	}()

	return tokens, nil
}

// GenerateWithTools implements tool calling
// 返回 *agentllm.ToolCallResponse 以符合 llm.ToolCallingClient 接口
func (p *GeminiProvider) GenerateWithTools(ctx context.Context, prompt string, tools []interfaces.Tool) (*agentllm.ToolCallResponse, error) {
	// Convert tools to Gemini function declarations
	functionDeclarations := p.convertToolsToFunctions(tools)

	// Configure model with tools using common.BaseProvider
	modelName := p.GetModel("")
	maxTokens := p.GetMaxTokens(0)
	temperature := p.GetTemperature(0)

	model := p.client.GenerativeModel(modelName)
	tempFloat32 := float32(temperature)
	model.Temperature = &tempFloat32

	// Validate maxTokens to prevent overflow
	if maxTokens > 0x7FFFFFFF { // Max int32
		maxTokens = 0x7FFFFFFF
	}
	maxTokensInt32 := int32(maxTokens)
	model.MaxOutputTokens = &maxTokensInt32
	model.Tools = []*genai.Tool{
		{FunctionDeclarations: functionDeclarations},
	}

	// Start chat with tools
	cs := model.StartChat()

	// Send message
	finalPrompt := prompt
	if p.Config.SystemPrompt != "" {
		finalPrompt = fmt.Sprintf("System: %s\n\nUser: %s", p.Config.SystemPrompt, prompt)
	}
	resp, err := cs.SendMessage(ctx, genai.Text(finalPrompt))
	if err != nil {
		return nil, agentErrors.NewErrorWithCause(agentErrors.CodeExternalService, "gemini tool calling API call failed", err).
			WithComponent("gemini").
			WithOperation("tool_calling").
			WithContext("model", modelName)
	}

	if len(resp.Candidates) == 0 {
		return nil, agentErrors.NewError(agentErrors.CodeExternalService, "no candidates returned from gemini").
			WithComponent("gemini").
			WithOperation("tool_calling").
			WithContext("model", modelName)
	}

	// Convert to llm.ToolCallResponse format (符合接口定义)
	result := &agentllm.ToolCallResponse{}

	// Process response parts
	for _, part := range resp.Candidates[0].Content.Parts {
		switch v := part.(type) {
		case genai.Text:
			result.Content += string(v)
		case *genai.FunctionCall:
			// Convert function call to our format - 转换为 agentllm.ToolCall 格式
			// 将参数序列化为 JSON 字符串
			argsJSON, _ := json.Marshal(v.Args)

			result.ToolCalls = append(result.ToolCalls, agentllm.ToolCall{
				ID:   common.GenerateCallID(),
				Type: "function",
				Function: struct {
					Name      string `json:"name"`
					Arguments string `json:"arguments"`
				}{
					Name:      v.Name,
					Arguments: string(argsJSON), // 保持 JSON 字符串格式
				},
			})
		}
	}

	return result, nil
}

// StreamWithTools implements streaming tool calls
// 返回 <-chan agentllm.ToolChunk 以符合 llm.ToolCallingClient 接口
func (p *GeminiProvider) StreamWithTools(ctx context.Context, prompt string, tools []interfaces.Tool) (<-chan agentllm.ToolChunk, error) {
	chunks := make(chan agentllm.ToolChunk, 100)

	// Convert tools to Gemini function declarations
	functionDeclarations := p.convertToolsToFunctions(tools)

	// Configure model with tools using common.BaseProvider
	modelName := p.GetModel("")
	maxTokens := p.GetMaxTokens(0)
	temperature := p.GetTemperature(0)

	model := p.client.GenerativeModel(modelName)
	tempFloat32 := float32(temperature)
	model.Temperature = &tempFloat32
	if maxTokens > 0x7FFFFFFF { // Max int32
		maxTokens = 0x7FFFFFFF
	}
	maxTokensInt32 := int32(maxTokens)
	model.MaxOutputTokens = &maxTokensInt32
	model.Tools = []*genai.Tool{
		{FunctionDeclarations: functionDeclarations},
	}

	// Start chat with tools
	cs := model.StartChat()

	go func() {
		defer close(chunks)

		var iter *genai.GenerateContentResponseIterator
		if p.Config.SystemPrompt != "" {
			// Prepend system prompt to the prompt for streaming with tools
			iter = cs.SendMessageStream(ctx, genai.Text(fmt.Sprintf("System: %s\n\nUser: %s", p.Config.SystemPrompt, prompt)))
		} else {
			iter = cs.SendMessageStream(ctx, genai.Text(prompt))
		}
		for {
			resp, err := iter.Next()
			if errors.Is(err, iterator.Done) {
				break
			}
			if err != nil {
				select {
				case chunks <- agentllm.ToolChunk{Type: "error", Value: err}:
					// Successfully sent
				case <-ctx.Done():
					// Context cancelled, exit immediately
				}
				break
			}

			// Process response parts
			for _, part := range resp.Candidates[0].Content.Parts {
				switch v := part.(type) {
				case genai.Text:
					select {
					case chunks <- agentllm.ToolChunk{Type: "content", Value: string(v)}:
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}
				case *genai.FunctionCall:
					select {
					case chunks <- agentllm.ToolChunk{Type: "tool_name", Value: v.Name}:
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}

					// 将参数序列化为 JSON 字符串��送
					argsJSON, _ := json.Marshal(v.Args)
					select {
					case chunks <- agentllm.ToolChunk{
						Type:  "tool_args",
						Value: string(argsJSON),
					}:
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}

					// Send complete tool call - 使用 agentllm.ToolCall 格式
					select {
					case chunks <- agentllm.ToolChunk{
						Type: "tool_call",
						Value: &agentllm.ToolCall{
							ID:   common.GenerateCallID(),
							Type: "function",
							Function: struct {
								Name      string `json:"name"`
								Arguments string `json:"arguments"`
							}{
								Name:      v.Name,
								Arguments: string(argsJSON), // 保持 JSON 字符串格式
							},
						},
					}:
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}
				}
			}
		}
	}()

	return chunks, nil
}

// Embed generates embeddings for text
func (p *GeminiProvider) Embed(ctx context.Context, text string) ([]float64, error) {
	// Gemini SDK doesn't expose EmbedContent method directly
	// This is a workaround - in production you should use the embedding API endpoint
	// For now, return a mock embedding
	mockEmbedding := make([]float64, 768)
	for i := range mockEmbedding {
		mockEmbedding[i] = float64(i) / 768.0
	}
	return mockEmbedding, nil
}

// Provider returns the provider type
func (p *GeminiProvider) Provider() constants.Provider {
	return constants.ProviderGemini
}

// ProviderName returns the provider name as a string
func (p *GeminiProvider) ProviderName() string {
	return string(constants.ProviderGemini)
}

// IsAvailable checks if the provider is available
func (p *GeminiProvider) IsAvailable() bool {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	// Try a simple completion to check availability
	cs := p.model.StartChat()
	_, err := cs.SendMessage(ctx, genai.Text("test"))

	return err == nil
}

// ModelName returns the model name
func (p *GeminiProvider) ModelName() string {
	return p.GetModel("")
}

// MaxTokens returns the max tokens setting
func (p *GeminiProvider) MaxTokens() int {
	return p.GetMaxTokens(0)
}

// convertToolsToFunctions converts our tools to Gemini function format
func (p *GeminiProvider) convertToolsToFunctions(tools []interfaces.Tool) []*genai.FunctionDeclaration {
	functions := make([]*genai.FunctionDeclaration, len(tools))

	for i, tool := range tools {
		functions[i] = &genai.FunctionDeclaration{
			Name:        tool.Name(),
			Description: tool.Description(),
			Parameters:  p.toolSchemaToGeminiSchema(tool.ArgsSchema()),
		}
	}

	return functions
}

// toolSchemaToGeminiSchema converts tool schema to Gemini schema format
func (p *GeminiProvider) toolSchemaToGeminiSchema(schema interface{}) *genai.Schema {
	// 处理不同类型的 schema 输入
	var schemaMap map[string]interface{}

	switch s := schema.(type) {
	case string:
		// JSON 字符串格式的 schema
		if s != "" {
			if err := json.Unmarshal([]byte(s), &schemaMap); err != nil {
				// 解析失败，返回空 schema
				return &genai.Schema{
					Type:       genai.TypeObject,
					Properties: map[string]*genai.Schema{},
				}
			}
		}
	case map[string]interface{}:
		schemaMap = s
	}

	if schemaMap == nil {
		return &genai.Schema{
			Type:       genai.TypeObject,
			Properties: map[string]*genai.Schema{},
		}
	}

	// 转换为 Gemini Schema 格式
	return p.convertToGeminiSchema(schemaMap)
}

// convertToGeminiSchema 递归转换 JSON Schema 到 Gemini Schema
func (p *GeminiProvider) convertToGeminiSchema(schemaMap map[string]interface{}) *genai.Schema {
	result := &genai.Schema{}

	// 处理类型
	if typeStr, ok := schemaMap["type"].(string); ok {
		switch typeStr {
		case "object":
			result.Type = genai.TypeObject
		case "string":
			result.Type = genai.TypeString
		case "number":
			result.Type = genai.TypeNumber
		case "integer":
			result.Type = genai.TypeInteger
		case "boolean":
			result.Type = genai.TypeBoolean
		case "array":
			result.Type = genai.TypeArray
		default:
			result.Type = genai.TypeString
		}
	}

	// 处理描述
	if desc, ok := schemaMap["description"].(string); ok {
		result.Description = desc
	}

	// 处理 properties
	if props, ok := schemaMap["properties"].(map[string]interface{}); ok {
		result.Properties = make(map[string]*genai.Schema)
		for key, val := range props {
			if propMap, ok := val.(map[string]interface{}); ok {
				result.Properties[key] = p.convertToGeminiSchema(propMap)
			}
		}
	}

	// 处理 required
	if required, ok := schemaMap["required"].([]interface{}); ok {
		for _, r := range required {
			if rStr, ok := r.(string); ok {
				result.Required = append(result.Required, rStr)
			}
		}
	}

	// 处理 items（数组类型）
	if items, ok := schemaMap["items"].(map[string]interface{}); ok {
		result.Items = p.convertToGeminiSchema(items)
	}

	return result
}

// GeminiStreamingProvider extends GeminiProvider with advanced streaming
type GeminiStreamingProvider struct {
	*GeminiProvider
}

// NewGeminiStreamingWithOptions creates a streaming-optimized provider using options pattern
func NewGeminiStreamingWithOptions(opts ...agentllm.ClientOption) (*GeminiStreamingProvider, error) {
	base, err := NewGeminiWithOptions(opts...)
	if err != nil {
		return nil, err
	}

	return &GeminiStreamingProvider{
		GeminiProvider: base,
	}, nil
}

// NewGeminiStreaming creates a streaming-optimized provider
func NewGeminiStreaming(config *agentllm.LLMOptions) (*GeminiStreamingProvider, error) {
	base, err := NewGeminiWithOptions(common.ConfigToOptions(config)...)
	if err != nil {
		return nil, err
	}

	return &GeminiStreamingProvider{
		GeminiProvider: base,
	}, nil
}

// StreamWithContext streams with cancellation support
func (p *GeminiStreamingProvider) StreamWithContext(ctx context.Context, prompt string) (<-chan StreamEvent, error) {
	events := make(chan StreamEvent, 100)

	cs := p.model.StartChat()

	go func() {
		defer close(events)

		// Send start event
		select {
		case events <- StreamEvent{
			Type:      "start",
			Timestamp: time.Now(),
		}:
			// Successfully sent
		case <-ctx.Done():
			// Context cancelled, exit immediately
			return
		}

		iter := cs.SendMessageStream(ctx, genai.Text(prompt))
		tokenCount := 0

		for {
			resp, err := iter.Next()
			if errors.Is(err, iterator.Done) {
				// Send completion event
				select {
				case events <- StreamEvent{
					Type:      "complete",
					Timestamp: time.Now(),
					Metadata: map[string]interface{}{
						"total_tokens": tokenCount,
					},
				}:
					// Successfully sent
				case <-ctx.Done():
					// Context cancelled, exit immediately
				}
				break
			}
			if err != nil {
				select {
				case events <- StreamEvent{
					Type:      "error",
					Error:     err,
					Timestamp: time.Now(),
				}:
					// Successfully sent
				case <-ctx.Done():
					// Context cancelled, exit immediately
				}
				break
			}

			// Extract and send content
			for _, part := range resp.Candidates[0].Content.Parts {
				if text, ok := part.(genai.Text); ok {
					tokenCount++
					select {
					case events <- StreamEvent{
						Type:      "token",
						Content:   string(text),
						Timestamp: time.Now(),
						Metadata: map[string]interface{}{
							"index": tokenCount,
						},
					}:
						// Successfully sent
					case <-ctx.Done():
						// Context cancelled, exit immediately
						return
					}
				}
			}
		}
	}()

	return events, nil
}

// StreamEvent represents a streaming event
type StreamEvent struct {
	Type      string // "start", "token", "error", "complete"
	Content   string
	Error     error
	Timestamp time.Time
	Metadata  map[string]interface{}
}
