package main

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"github.com/kart-io/goagent/interfaces"
	"github.com/kart-io/goagent/llm"
	"github.com/kart-io/goagent/llm/providers"
	"github.com/kart-io/goagent/tools"
	"github.com/kart-io/goagent/utils/json"
)

func main() {
	fmt.Println("=== Ollama LLM Example ===")
	fmt.Println()

	// ç¤ºä¾‹ 1: åŸºæœ¬ Ollama å®¢æˆ·ç«¯ä½¿ç”¨
	basicOllamaExample()

	// ç¤ºä¾‹ 2: ä½¿ç”¨ Ollama çš„ Chat æ–¹æ³•
	ollamaAgentExample()

	// ç¤ºä¾‹ 3: ä½¿ç”¨ Complete æ–¹æ³•
	ollamaWithCompletionExample()

	// ç¤ºä¾‹ 4: åˆ—å‡ºå¯ç”¨æ¨¡å‹
	listOllamaModels()

	// ç¤ºä¾‹ 5: ä½¿ç”¨ Ollama LLM è°ƒç”¨å·¥å…·ï¼ˆgetCurrentTimeToolï¼‰
	ollamaWithToolsExample()
}

// basicOllamaExample æ¼”ç¤ºåŸºæœ¬çš„ Ollama å®¢æˆ·ç«¯ä½¿ç”¨
func basicOllamaExample() {
	fmt.Println("1. Basic Ollama Client Usage")
	fmt.Println("----------------------------")

	// åˆ›å»º Ollama å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
	client, err := providers.NewOllamaClientSimple("gemma3:12b")
	if err != nil {
		log.Printf("Error creating Ollama client: %v\n", err)
		return
	}

	// æ£€æŸ¥ Ollama æ˜¯å¦å¯ç”¨
	if !client.IsAvailable() {
		fmt.Println("âŒ Ollama is not available. Please ensure Ollama is running on http://localhost:11434")
		fmt.Println("   Install Ollama: https://ollama.ai/")
		fmt.Println("   Start Ollama: ollama serve")
		fmt.Println("   Pull a model: ollama pull gemma3:12b")
		return
	}

	fmt.Println("âœ… Ollama is available")

	// ç®€å•å¯¹è¯
	ctx := context.Background()
	messages := []llm.Message{
		llm.SystemMessage("You are a helpful assistant."),
		llm.UserMessage("What is Go programming language in one sentence?"),
	}

	response, err := client.Chat(ctx, messages)
	if err != nil {
		log.Printf("Error: %v\n", err)
		return
	}

	fmt.Printf("Response: %s\n", response.Content)
	fmt.Printf("Model: %s\n", response.Model)
	fmt.Printf("Tokens used: %d\n", response.TokensUsed)
	fmt.Println()
}

// ollamaAgentExample æ¼”ç¤ºä½¿ç”¨ Ollama åˆ›å»ºç®€å•å¯¹è¯
func ollamaAgentExample() {
	fmt.Println("2. Ollama Chat Example")
	fmt.Println("-----------------------")

	// åˆ›å»º Ollama å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ ‡å‡†é…ç½®ï¼‰
	ollamaClient, err := providers.NewOllamaWithOptions(
		llm.WithModel("gemma3:12b"), // æˆ–è€…ä½¿ç”¨å…¶ä»–æ¨¡å‹å¦‚ "mistral", "codellama", "phi"
		llm.WithTemperature(0.7),
		llm.WithMaxTokens(1000),
	)
	if err != nil {
		log.Printf("Error creating Ollama client: %v\n", err)
		return
	}

	// æ£€æŸ¥å¯ç”¨æ€§
	if !ollamaClient.IsAvailable() {
		fmt.Println("âŒ Ollama not available. Skipping this example.")
		return
	}

	// ç›´æ¥ä½¿ç”¨ Chat æ–¹æ³•
	ctx := context.Background()
	messages := []llm.Message{
		llm.SystemMessage("You are a helpful AI assistant powered by Ollama. Be concise and clear."),
		llm.UserMessage("Explain Docker in 2 sentences."),
	}

	response, err := ollamaClient.Chat(ctx, messages)
	if err != nil {
		log.Printf("Chat failed: %v\n", err)
		return
	}

	fmt.Printf("Ollama Response: %s\n", response.Content)
	fmt.Printf("Model used: %s\n", response.Model)
	fmt.Println()
}

// ollamaWithCompletionExample æ¼”ç¤ºä½¿ç”¨ Complete æ–¹æ³•
func ollamaWithCompletionExample() {
	fmt.Println("3. Ollama Completion Example")
	fmt.Println("-----------------------------")

	// åˆ›å»º Ollama å®¢æˆ·ç«¯
	ollamaClient, err := providers.NewOllamaClientSimple("gemma3:12b")
	if err != nil {
		log.Printf("Failed to create Ollama client: %v\n", err)
		return
	}

	if !ollamaClient.IsAvailable() {
		fmt.Println("âŒ Ollama not available. Skipping this example.")
		return
	}

	// ä½¿ç”¨ Complete æ–¹æ³•
	ctx := context.Background()
	req := &llm.CompletionRequest{
		Messages: []llm.Message{
			llm.SystemMessage("You are a helpful assistant that provides clear, concise answers."),
			llm.UserMessage("What is 25 * 4? Just give me the number."),
		},
		Temperature: 0.1, // ä½æ¸©åº¦è·å¾—æ›´ç¡®å®šçš„ç­”æ¡ˆ
		MaxTokens:   50,
	}

	response, err := ollamaClient.Complete(ctx, req)
	if err != nil {
		log.Printf("Completion failed: %v\n", err)
		return
	}

	fmt.Printf("Response: %s\n", response.Content)
	fmt.Println()
}

// listOllamaModels åˆ—å‡ºå¯ç”¨çš„ Ollama æ¨¡å‹
func listOllamaModels() {
	fmt.Println("4. Available Ollama Models")
	fmt.Println("--------------------------")

	client, err := providers.NewOllamaClientSimple("")
	if err != nil {
		log.Printf("Failed to create Ollama client: %v\n", err)
		return
	}

	if !client.IsAvailable() {
		fmt.Println("âŒ Ollama not available")
		return
	}

	models, err := client.ListModels()
	if err != nil {
		log.Printf("Failed to list models: %v\n", err)
		return
	}

	if len(models) == 0 {
		fmt.Println("No models installed. Pull a model first:")
		fmt.Println("  ollama pull gemma3:12b")
		fmt.Println("  ollama pull mistral")
		fmt.Println("  ollama pull codellama")
		return
	}

	fmt.Println("Available models:")
	for _, model := range models {
		fmt.Printf("  - %s\n", model)
	}
	fmt.Println()

	// æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
	fmt.Println("Example: Using different models")
	for _, modelName := range []string{"gemma3:12b", "mistral", "phi"} {
		// æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
		modelAvailable := false
		for _, m := range models {
			if strings.HasPrefix(m, modelName) {
				modelAvailable = true
				break
			}
		}

		if modelAvailable {
			fmt.Printf("\n  Using %s model:\n", modelName)
			client, err := providers.NewOllamaClientSimple(modelName)
			if err != nil {
				fmt.Printf("    Error creating client: %v\n", err)
				continue
			}
			ctx := context.Background()

			resp, err := client.Chat(ctx, []llm.Message{
				llm.UserMessage("Say hello in one word"),
			})

			if err != nil {
				fmt.Printf("    Error: %v\n", err)
			} else {
				fmt.Printf("    Response: %s\n", resp.Content)
			}
		}
	}
}

// é¢å¤–çš„è¾…åŠ©å‡½æ•°ï¼šæ‹‰å–æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰ - kept for reference
/*
func pullModelIfNeeded(modelName string) error {
	client := providers.NewOllamaClientSimple(modelName)

	// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
	models, err := client.ListModels()
	if err != nil {
		return err
	}

	for _, m := range models {
		if strings.HasPrefix(m, modelName) {
			return nil // æ¨¡å‹å·²å­˜åœ¨
		}
	}

	// æ‹‰å–æ¨¡å‹
	fmt.Printf("Pulling model %s... (this may take a while)\n", modelName)
	return client.PullModel(modelName)
}
*/

// getCurrentTimeTool è·å–å½“å‰æ—¶é—´çš„å·¥å…·
func getCurrentTimeTool() interfaces.Tool {
	return tools.NewBaseTool(
		"get_current_time",
		"Get the current local time in format YYYY-MM-DD HH:MM:SS",
		`{
			"type": "object",
			"properties": {},
			"required": []
		}`,
		func(ctx context.Context, input *interfaces.ToolInput) (*interfaces.ToolOutput, error) {
			currentTime := time.Now().Format("2006-01-02 15:04:05")
			return &interfaces.ToolOutput{
				Result:  currentTime,
				Success: true,
			}, nil
		},
	)
}

// ollamaWithToolsExample æ¼”ç¤ºä½¿ç”¨ Ollama LLM è°ƒç”¨å·¥å…·
func ollamaWithToolsExample() {
	fmt.Println("5. Ollama with Tools Example (getCurrentTimeTool)")
	fmt.Println("--------------------------------------------------")

	// åˆ›å»º Ollama å®¢æˆ·ç«¯
	ollamaClient, err := providers.NewOllamaClientSimple("gemma3:12b")
	if err != nil {
		log.Printf("Failed to create Ollama client: %v\n", err)
		return
	}

	if !ollamaClient.IsAvailable() {
		fmt.Println("âŒ Ollama not available. Skipping this example.")
		return
	}

	// åˆ›å»ºè·å–å½“å‰æ—¶é—´çš„å·¥å…·
	timeTool := getCurrentTimeTool()

	// åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤©æ°”æŸ¥è¯¢å·¥å…·ä½œä¸ºå¯¹æ¯”
	weatherTool := tools.NewBaseTool(
		"get_weather",
		"Get the current weather for a given city",
		`{
			"type": "object",
			"properties": {
				"city": {
					"type": "string",
					"description": "The city name"
				}
			},
			"required": ["city"]
		}`,
		func(ctx context.Context, input *interfaces.ToolInput) (*interfaces.ToolOutput, error) {
			city, ok := input.Args["city"].(string)
			if !ok {
				return &interfaces.ToolOutput{
					Success: false,
					Error:   "city must be a string",
				}, nil
			}

			// æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
			weatherData := map[string]interface{}{
				"city":        city,
				"temperature": 25,
				"condition":   "Sunny",
				"humidity":    60,
			}

			return &interfaces.ToolOutput{
				Result:  weatherData,
				Success: true,
			}, nil
		},
	)

	fmt.Println("\nğŸ“Œ ç¤ºä¾‹ 1: ç›´æ¥è°ƒç”¨å·¥å…·")
	fmt.Println("-------------------------")

	// ç›´æ¥è°ƒç”¨æ—¶é—´å·¥å…·
	ctx := context.Background()
	timeInput := &interfaces.ToolInput{
		Args:    map[string]interface{}{},
		Context: ctx,
	}

	timeOutput, err := timeTool.Invoke(ctx, timeInput)
	if err != nil {
		log.Printf("Error invoking time tool: %v\n", err)
	} else {
		fmt.Printf("âœ… Current Time: %v\n", timeOutput.Result)
	}

	// ç›´æ¥è°ƒç”¨å¤©æ°”å·¥å…·
	weatherInput := &interfaces.ToolInput{
		Args: map[string]interface{}{
			"city": "Beijing",
		},
		Context: ctx,
	}

	weatherOutput, err := weatherTool.Invoke(ctx, weatherInput)
	if err != nil {
		log.Printf("Error invoking weather tool: %v\n", err)
	} else {
		weatherJSON, _ := json.MarshalIndent(weatherOutput.Result, "", "  ")
		fmt.Printf("âœ… Weather Data:\n%s\n", string(weatherJSON))
	}

	fmt.Println("\nğŸ“Œ ç¤ºä¾‹ 2: é€šè¿‡ LLM è°ƒç”¨å·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰")
	fmt.Println("-------------------------------------")

	// æ„å»ºå·¥å…·æè¿°ï¼Œç”¨äº LLM ç†è§£
	toolDescriptions := buildToolDescriptions([]interfaces.Tool{timeTool, weatherTool})

	// åˆ›å»ºåŒ…å«å·¥å…·ä¿¡æ¯çš„ prompt
	userQuery := "What time is it now?"
	messages := []llm.Message{
		llm.SystemMessage(fmt.Sprintf(`You are a helpful AI assistant with access to the following tools:

%s

When you need to use a tool, respond in this format:
Tool: <tool_name>
Input: <tool_input_as_json>

After receiving the tool result, provide a natural language response to the user.`, toolDescriptions)),
		llm.UserMessage(userQuery),
	}

	// è°ƒç”¨ LLM
	response, err := ollamaClient.Chat(ctx, messages)
	if err != nil {
		log.Printf("Error calling LLM: %v\n", err)
		return
	}

	fmt.Printf("ğŸ¤– LLM Response:\n%s\n\n", response.Content)

	// è§£æ LLM å“åº”ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
	if strings.Contains(response.Content, "Tool:") {
		toolName, toolInput := parseToolCall(response.Content)
		fmt.Printf("ğŸ”§ Detected Tool Call:\n")
		fmt.Printf("   Tool: %s\n", toolName)
		fmt.Printf("   Input: %s\n\n", toolInput)

		// æ‰§è¡Œå·¥å…·è°ƒç”¨
		var toolResult interface{}
		switch toolName {
		case "get_current_time":
			output, toolErr := timeTool.Invoke(ctx, &interfaces.ToolInput{
				Args:    map[string]interface{}{},
				Context: ctx,
			})
			if toolErr == nil {
				toolResult = output.Result
			}
		case "get_weather":
			var args map[string]interface{}
			if err := json.Unmarshal([]byte(toolInput), &args); err != nil {
				log.Printf("Error unmarshalling tool input: %v\n", err)
				return
			}
			output, toolErr := weatherTool.Invoke(ctx, &interfaces.ToolInput{
				Args:    args,
				Context: ctx,
			})
			if toolErr == nil {
				toolResult = output.Result
			}
		}

		if toolResult != nil {
			fmt.Printf("ğŸ¯ Tool Result: %v\n\n", toolResult)

			// å°†å·¥å…·ç»“æœè¿”å›ç»™ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
			messages = append(messages,
				llm.AssistantMessage(response.Content),
				llm.UserMessage(fmt.Sprintf("Tool Result: %v\n\nNow provide a natural language response to the user.", toolResult)),
			)

			finalResponse, finalErr := ollamaClient.Chat(ctx, messages)
			if finalErr != nil {
				log.Printf("Error getting final response: %v\n", finalErr)
				return
			}

			fmt.Printf("ğŸ’¬ Final Response:\n%s\n", finalResponse.Content)
		}
	}

	fmt.Println("\nğŸ“Œ ç¤ºä¾‹ 3: å¤šä¸ªå·¥å…·è°ƒç”¨åœºæ™¯")
	fmt.Println("-----------------------------")

	userQuery2 := "What's the weather in Shanghai?"
	messages2 := []llm.Message{
		llm.SystemMessage(fmt.Sprintf(`You are a helpful AI assistant with access to the following tools:

%s

When you need to use a tool, respond in this format:
Tool: <tool_name>
Input: <tool_input_as_json>`, toolDescriptions)),
		llm.UserMessage(userQuery2),
	}

	response2, err := ollamaClient.Chat(ctx, messages2)
	if err != nil {
		log.Printf("Error calling LLM: %v\n", err)
		return
	}

	fmt.Printf("ğŸ¤– LLM Response:\n%s\n\n", response2.Content)

	// è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
	if strings.Contains(response2.Content, "Tool:") {
		toolName, toolInput := parseToolCall(response2.Content)
		fmt.Printf("ğŸ”§ Detected Tool Call:\n")
		fmt.Printf("   Tool: %s\n", toolName)
		fmt.Printf("   Input: %s\n\n", toolInput)

		if toolName == "get_weather" {
			var args map[string]interface{}
			if err := json.Unmarshal([]byte(toolInput), &args); err != nil {
				log.Printf("Error unmarshalling tool input: %v\n", err)
				return
			}
			output, err := weatherTool.Invoke(ctx, &interfaces.ToolInput{
				Args:    args,
				Context: ctx,
			})
			if err == nil {
				weatherJSON, _ := json.MarshalIndent(output.Result, "", "  ")
				fmt.Printf("ğŸ¯ Tool Result:\n%s\n", string(weatherJSON))
			}
		}
	}

	fmt.Println()
}

// buildToolDescriptions æ„å»ºå·¥å…·æè¿°å­—ç¬¦ä¸²
func buildToolDescriptions(tools []interfaces.Tool) string {
	var descriptions []string
	for _, tool := range tools {
		desc := fmt.Sprintf("- %s: %s\n  Schema: %s",
			tool.Name(),
			tool.Description(),
			tool.ArgsSchema(),
		)
		descriptions = append(descriptions, desc)
	}
	return strings.Join(descriptions, "\n\n")
}

// parseToolCall è§£æ LLM å“åº”ä¸­çš„å·¥å…·è°ƒç”¨
func parseToolCall(response string) (toolName string, input string) {
	lines := strings.Split(response, "\n")
	for i, line := range lines {
		if strings.HasPrefix(line, "Tool:") {
			toolName = strings.TrimSpace(strings.TrimPrefix(line, "Tool:"))
		}
		if strings.HasPrefix(line, "Input:") {
			input = strings.TrimSpace(strings.TrimPrefix(line, "Input:"))
			// å¦‚æœè¾“å…¥è·¨å¤šè¡Œï¼Œåˆå¹¶å®ƒä»¬
			if i+1 < len(lines) && strings.HasPrefix(strings.TrimSpace(lines[i+1]), "{") {
				input = strings.TrimSpace(lines[i+1])
			}
		}
	}
	return
}
