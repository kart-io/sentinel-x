package main

import (
	"context"
	"fmt"
	"os"
	"strings"

	"github.com/kart-io/goagent/examples/advanced/multi-agent-modular/agents"
	"github.com/kart-io/goagent/llm"
	"github.com/kart-io/goagent/llm/constants"
	"github.com/kart-io/goagent/llm/providers"
	"github.com/kart-io/goagent/utils/json"
)

func main() {
	fmt.Println("=== Multi-Agent Collaboration System ===")
	fmt.Println("A modular implementation with specialized agents")
	fmt.Println("")

	// Initialize LLM client with Ollama priority
	llmClient := initializeLLM()

	// Create coordinator
	coordinator := agents.NewCoordinator(llmClient)

	// Example tasks
	tasks := []string{
		"Develop a microservices architecture for an e-commerce platform",
		"Optimize database performance for high-traffic application",
		"Implement CI/CD pipeline with security scanning",
	}

	ctx := context.Background()

	// Execute workflows
	for i, task := range tasks {
		fmt.Printf("\n%s\n", strings.Repeat("=", 60))
		fmt.Printf("Task %d: %s\n", i+1, task)
		fmt.Println(strings.Repeat("=", 60))

		result, err := coordinator.ExecuteWorkflow(ctx, task)
		if err != nil {
			fmt.Printf("âŒ Workflow failed: %v\n", err)
			continue
		}

		// Display results
		displayResults(result)
	}

	// Demonstrate parallel execution
	fmt.Printf("\n\n%s\n", strings.Repeat("=", 60))
	fmt.Println("PARALLEL EXECUTION DEMO")
	fmt.Println(strings.Repeat("=", 60))

	parallelTasks := []string{
		"Analyze system logs for anomalies",
		"Monitor API performance metrics",
	}

	results, err := coordinator.ExecuteParallel(ctx, parallelTasks)
	if err != nil {
		fmt.Printf("âŒ Parallel execution had errors: %v\n", err)
	}

	for i, result := range results {
		if result != nil {
			fmt.Printf("\nğŸ“Š Parallel Task %d Results:\n", i+1)
			fmt.Printf("   Status: %s\n", result.Status)
			fmt.Printf("   Duration: %s\n", result.Duration)
		}
	}

	// Demonstrate custom workflow
	fmt.Printf("\n\n%s\n", strings.Repeat("=", 60))
	fmt.Println("CUSTOM WORKFLOW DEMO")
	fmt.Println(strings.Repeat("=", 60))

	customSequence := []string{"analysis", "analysis", "strategy", "execution"}
	fmt.Printf("Custom sequence: %v\n", customSequence)

	customResult, err := coordinator.ExecuteCustomWorkflow(
		ctx,
		"Implement real-time data processing pipeline",
		customSequence,
	)
	if err != nil {
		fmt.Printf("âŒ Custom workflow failed: %v\n", err)
	} else {
		fmt.Printf("âœ… Custom workflow completed: %s\n", customResult.Status)
	}

	fmt.Printf("\n\n%s\n", strings.Repeat("=", 60))
	fmt.Println("âœ¨ Multi-Agent Collaboration Complete!")
	fmt.Println(strings.Repeat("=", 60))
}

// initializeLLM sets up the LLM client with priority for Ollama
func initializeLLM() llm.Client {
	// Priority 1: Ollama (local, no API key needed)
	ollamaModel := os.Getenv("OLLAMA_MODEL")
	if ollamaModel == "" {
		ollamaModel = "llama2"
	}

	fmt.Printf("Checking Ollama with model '%s'...\n", ollamaModel)
	ollamaClient, err := providers.NewOllamaClientSimple(ollamaModel)
	if err != nil {
		fmt.Printf("âš ï¸  Error creating Ollama client: %v\n", err)
		return nil
	}
	if ollamaClient.IsAvailable() {
		fmt.Printf("âœ“ Using Ollama provider with model: %s\n\n", ollamaModel)
		return ollamaClient
	}

	fmt.Println("Ollama not available, checking cloud providers...")

	// Priority 2: OpenAI
	if apiKey := os.Getenv("OPENAI_API_KEY"); apiKey != "" {
		client, err := providers.NewOpenAIWithOptions(
			llm.WithAPIKey(apiKey),
			llm.WithModel("gpt-3.5-turbo"),
			llm.WithMaxTokens(1000),
			llm.WithTemperature(0.7),
		)
		if err == nil && client.IsAvailable() {
			fmt.Println("âœ“ Using OpenAI provider")
			return client
		}
	}

	// Priority 3: Gemini
	if apiKey := os.Getenv("GEMINI_API_KEY"); apiKey != "" {
		client, err := providers.NewGeminiWithOptions(
			llm.WithAPIKey(apiKey),
			llm.WithModel("gemini-pro"),
			llm.WithMaxTokens(1000),
			llm.WithTemperature(0.7),
		)
		if err == nil && client.IsAvailable() {
			fmt.Println("âœ“ Using Gemini provider")
			return client
		}
	}

	// Priority 4: Mock client for demonstration
	fmt.Println("âš ï¸  No real LLM available, using mock client for demonstration")
	return &MockLLMClient{}
}

// displayResults shows workflow results in a formatted way
func displayResults(result *agents.WorkflowResult) {
	fmt.Println("\nğŸ“Š Workflow Results")
	fmt.Println(strings.Repeat("-", 40))
	fmt.Printf("Status: %s\n", result.Status)
	fmt.Printf("Duration: %s\n", result.Duration)

	if result.Analysis != "" {
		fmt.Printf("\nğŸ” Analysis:\n%s\n", truncate(result.Analysis, 200))
	}

	if result.Strategy != "" {
		fmt.Printf("\nğŸ“‹ Strategy:\n%s\n", truncate(result.Strategy, 200))
	}

	if result.Execution != "" {
		fmt.Printf("\nâš¡ Execution:\n%s\n", truncate(result.Execution, 200))
	}

	if len(result.ToolResults) > 0 {
		fmt.Println("\nğŸ› ï¸  Tool Results:")
		for tool, res := range result.ToolResults {
			jsonData, _ := json.MarshalIndent(res, "  ", "  ")
			fmt.Printf("  %s:\n  %s\n", tool, truncate(string(jsonData), 150))
		}
	}

	fmt.Printf("\nğŸ“ Summary:\n%s\n", result.Summary)
}

// truncate helper function
func truncate(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}

// MockLLMClient for demonstration when no real LLM is available
type MockLLMClient struct{}

func (m *MockLLMClient) Complete(ctx context.Context, req *llm.CompletionRequest) (*llm.CompletionResponse, error) {
	return &llm.CompletionResponse{
		Content:  "Mock response for demonstration",
		Provider: "mock",
		Model:    "mock-model",
	}, nil
}

func (m *MockLLMClient) Chat(ctx context.Context, messages []llm.Message) (*llm.CompletionResponse, error) {
	if len(messages) == 0 {
		return nil, fmt.Errorf("no messages provided")
	}

	// Generate contextual mock responses
	lastMessage := messages[len(messages)-1].Content
	content := "Mock response: Processing request..."

	if contains(lastMessage, "Analysis Agent") || contains(lastMessage, "analyze") {
		content = `Analysis Results:
â€¢ Identified 5 key patterns in the data
â€¢ Complexity: Medium-High
â€¢ Risks: Scalability, data consistency
â€¢ Opportunities: Caching, parallel processing
â€¢ Recommended approach: Phased implementation`
	} else if contains(lastMessage, "Strategy Agent") || contains(lastMessage, "strategy") {
		content = `Strategic Plan:
â€¢ Phase 1: Assessment and preparation (Week 1)
â€¢ Phase 2: Core implementation (Week 2-3)
â€¢ Phase 3: Testing and optimization (Week 4)
â€¢ Resources: Development team, infrastructure
â€¢ Success metrics: Performance improvement by 50%`
	} else if contains(lastMessage, "Execution Agent") || contains(lastMessage, "execute") {
		content = `Execution Plan:
â€¢ Step 1: Set up monitoring
â€¢ Step 2: Deploy core components
â€¢ Step 3: Configure load balancing
â€¢ Step 4: Implement caching layer
â€¢ Step 5: Validate and test`
	}

	return &llm.CompletionResponse{
		Content:  content,
		Provider: "mock",
		Model:    "mock-model",
	}, nil
}

func (m *MockLLMClient) Provider() constants.Provider {
	return constants.ProviderCustom
}

func (m *MockLLMClient) IsAvailable() bool {
	return true
}

// contains checks if a string contains a substring (case-insensitive)
func contains(s, substr string) bool {
	return len(s) > 0 && len(substr) > 0 &&
		(s == substr || len(s) > len(substr) &&
			(s[:len(substr)] == substr || s[len(s)-len(substr):] == substr ||
				findSubstring(s, substr)))
}

// findSubstring helper for contains
func findSubstring(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}
