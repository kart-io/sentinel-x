// Package providers contains LLM provider implementations.
//
// This file serves as a TEMPLATE for implementing new LLM providers.
// Copy this file and replace "Template" with your provider name.
//
// Steps to create a new provider:
//  1. Copy this file to providers/<provider_name>.go
//  2. Replace "Template" with your provider name (e.g., "Claude", "Mistral")
//  3. Add provider constants in llm/constants/providers.go
//  4. Implement the required methods
//  5. Add capability initialization in the constructor
//  6. Create tests in providers/<provider_name>_test.go
//  7. Register the provider in the factory if needed
package providers

import (
	"bufio"
	"context"
	"fmt"
	"io"
	"strings"
	"time"

	"github.com/go-resty/resty/v2"

	agentErrors "github.com/kart-io/goagent/errors"
	"github.com/kart-io/goagent/interfaces"
	agentllm "github.com/kart-io/goagent/llm"
	"github.com/kart-io/goagent/llm/constants"
	"github.com/kart-io/goagent/utils/httpclient"
	"github.com/kart-io/goagent/utils/json"
)

// ============================================================================
// STEP 1: Define the provider struct
// ============================================================================

// TemplateProvider implements LLM interface for the Template API.
// Replace "Template" with your provider name.
type TemplateProvider struct {
	// Embed BaseProvider for common functionality
	*BaseProvider
	// Embed ProviderCapabilities for capability checking
	*ProviderCapabilities
	// HTTP client for API calls
	client *httpclient.Client
	// Provider-specific fields
	apiKey  string
	baseURL string
}

// ============================================================================
// STEP 2: Define request/response types
// ============================================================================

// TemplateRequest represents a request to the Template API.
// Customize fields based on the provider's API specification.
type TemplateRequest struct {
	Model       string            `json:"model"`
	Messages    []TemplateMessage `json:"messages"`
	Temperature float64           `json:"temperature,omitempty"`
	MaxTokens   int               `json:"max_tokens,omitempty"`
	TopP        float64           `json:"top_p,omitempty"`
	Stream      bool              `json:"stream,omitempty"`
	Stop        []string          `json:"stop,omitempty"`
	// Add provider-specific fields here
}

// TemplateMessage represents a message in Template format.
type TemplateMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
	Name    string `json:"name,omitempty"`
}

// TemplateResponse represents a response from the Template API.
type TemplateResponse struct {
	ID      string           `json:"id"`
	Object  string           `json:"object"`
	Created int64            `json:"created"`
	Model   string           `json:"model"`
	Choices []TemplateChoice `json:"choices"`
	Usage   TemplateUsage    `json:"usage"`
}

// TemplateChoice represents a choice in the response.
type TemplateChoice struct {
	Index        int             `json:"index"`
	Message      TemplateMessage `json:"message"`
	FinishReason string          `json:"finish_reason"`
}

// TemplateUsage represents token usage.
type TemplateUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// TemplateStreamResponse represents a streaming response.
type TemplateStreamResponse struct {
	ID      string           `json:"id"`
	Object  string           `json:"object"`
	Created int64            `json:"created"`
	Model   string           `json:"model"`
	Choices []TemplateChoice `json:"choices"`
}

// TemplateErrorResponse represents an error response.
type TemplateErrorResponse struct {
	Error struct {
		Message string `json:"message"`
		Type    string `json:"type"`
		Code    string `json:"code"`
	} `json:"error"`
}

// ============================================================================
// STEP 3: Implement constructors
// ============================================================================

// Define provider-specific constants (add these to llm/constants/providers.go):
// const (
//     ProviderTemplate    Provider = "template"
//     TemplateBaseURL              = "https://api.template.com/v1"
//     TemplateDefaultModel         = "template-model-v1"
//     EnvTemplateAPIKey            = "TEMPLATE_API_KEY"
//     EnvTemplateBaseURL           = "TEMPLATE_BASE_URL"
//     EnvTemplateModel             = "TEMPLATE_MODEL"
// )

// NewTemplateWithOptions creates a new Template provider using options pattern.
// This is the preferred constructor.
func NewTemplateWithOptions(opts ...agentllm.ClientOption) (*TemplateProvider, error) {
	// Create BaseProvider with unified options handling
	base := NewBaseProvider(opts...)

	// Apply provider-specific defaults
	// Replace with actual constants from llm/constants/providers.go
	base.ApplyProviderDefaults(
		constants.ProviderCustom, // Replace with your provider constant
		"https://api.template.com/v1",
		"template-model-v1",
		"TEMPLATE_BASE_URL",
		"TEMPLATE_MODEL",
	)

	// Ensure API key is set
	if err := base.EnsureAPIKey("TEMPLATE_API_KEY", constants.ProviderCustom); err != nil {
		return nil, err
	}

	// Create HTTP client using BaseProvider's helper
	client := base.NewHTTPClient(HTTPClientConfig{
		Timeout: base.GetTimeout(),
		Headers: map[string]string{
			constants.HeaderContentType:   constants.ContentTypeJSON,
			constants.HeaderAccept:        constants.ContentTypeJSON,
			constants.HeaderAuthorization: constants.AuthBearerPrefix + base.Config.APIKey,
			// Add provider-specific headers here
		},
		BaseURL: base.Config.BaseURL,
	})

	provider := &TemplateProvider{
		BaseProvider: base,
		// Initialize capabilities based on what your provider supports
		ProviderCapabilities: NewProviderCapabilities(
			agentllm.CapabilityCompletion,
			agentllm.CapabilityChat,
			agentllm.CapabilityStreaming,
			// Add more capabilities as supported:
			// agentllm.CapabilityToolCalling,
			// agentllm.CapabilityEmbedding,
			// agentllm.CapabilityVision,
			// agentllm.CapabilityJSON,
		),
		client:  client,
		apiKey:  base.Config.APIKey,
		baseURL: base.Config.BaseURL,
	}

	return provider, nil
}

// ============================================================================
// STEP 4: Implement Client interface (required)
// ============================================================================

// Complete implements basic text completion.
func (p *TemplateProvider) Complete(ctx context.Context, req *agentllm.CompletionRequest) (*agentllm.CompletionResponse, error) {
	// Convert messages to provider format using shared utility
	// Option 1: Use ConvertMessages with custom converter
	messages := ConvertMessages(req.Messages, func(msg agentllm.Message) TemplateMessage {
		return TemplateMessage{
			Role:    msg.Role,
			Content: msg.Content,
			Name:    msg.Name,
		}
	})

	// Option 2: If using StandardMessage format (OpenAI-compatible)
	// messages := ConvertToStandardMessages(req.Messages)

	// Option 3: If provider needs role mapping (like Cohere)
	// messages := ConvertMessagesWithRoleMapping(req.Messages,
	//     func(role string) string {
	//         switch role {
	//         case "user": return "USER"
	//         case "assistant": return "CHATBOT"
	//         default: return "USER"
	//         }
	//     },
	//     func(msg agentllm.Message, mappedRole string) TemplateMessage {
	//         return TemplateMessage{Role: mappedRole, Message: msg.Content}
	//     })

	// Option 4: If provider expects single prompt string (like HuggingFace)
	// prompt := MessagesToPrompt(req.Messages, DefaultPromptFormatter)

	// Build request using BaseProvider helpers
	model := p.GetModel(req.Model)
	templateReq := TemplateRequest{
		Model:       model,
		Messages:    messages,
		Temperature: p.GetTemperature(req.Temperature),
		MaxTokens:   p.GetMaxTokens(req.MaxTokens),
		TopP:        req.TopP,
		Stop:        req.Stop,
		Stream:      false,
	}

	// Execute with retry using shared retry logic
	resp, err := p.executeWithRetry(ctx, &templateReq)
	if err != nil {
		return nil, err
	}

	// Convert response
	return p.convertResponse(resp), nil
}

// Chat implements chat conversation.
func (p *TemplateProvider) Chat(ctx context.Context, messages []agentllm.Message) (*agentllm.CompletionResponse, error) {
	return p.Complete(ctx, &agentllm.CompletionRequest{
		Messages: messages,
	})
}

// Provider returns the provider type.
func (p *TemplateProvider) Provider() constants.Provider {
	return constants.ProviderCustom // Replace with your provider constant
}

// IsAvailable checks if the provider is available.
func (p *TemplateProvider) IsAvailable() bool {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	_, err := p.Complete(ctx, &agentllm.CompletionRequest{
		Messages: []agentllm.Message{{Role: constants.RoleUser, Content: "test"}},
	})

	return err == nil
}

// ============================================================================
// STEP 5: Implement optional interfaces
// ============================================================================

// Stream implements simple token streaming (TokenStreamingClient interface).
func (p *TemplateProvider) Stream(ctx context.Context, prompt string) (<-chan string, error) {
	tokens := make(chan string, 100)

	model := p.GetModel("")
	templateReq := &TemplateRequest{
		Model:       model,
		Messages:    []TemplateMessage{{Role: constants.RoleUser, Content: prompt}},
		Temperature: p.GetTemperature(0),
		MaxTokens:   p.GetMaxTokens(0),
		Stream:      true,
	}

	// Execute streaming request
	resp, err := p.client.R().
		SetContext(ctx).
		SetHeader(constants.HeaderAccept, constants.ContentTypeEventStream).
		SetBody(templateReq).
		SetDoNotParseResponse(true).
		Post(p.baseURL + "/chat/completions")

	if err != nil {
		return nil, agentErrors.NewLLMRequestError(p.ProviderName(), model, err)
	}

	if !resp.IsSuccess() {
		resp.RawBody().Close()
		return nil, p.handleHTTPError(resp, model)
	}

	// Start goroutine to read stream
	go func() {
		defer close(tokens)
		defer resp.RawBody().Close()

		reader := bufio.NewReader(resp.RawBody())
		for {
			line, err := reader.ReadString('\n')
			if err != nil {
				if err != io.EOF {
					fmt.Printf("Stream read error: %v\n", err)
				}
				return
			}

			// Skip empty lines and "data: " prefix
			line = strings.TrimSpace(line)
			if line == "" || line == "data: [DONE]" {
				continue
			}
			if strings.HasPrefix(line, "data: ") {
				line = strings.TrimPrefix(line, "data: ")
			}

			// Parse streaming response
			var streamResp TemplateStreamResponse
			if err := json.Unmarshal([]byte(line), &streamResp); err != nil {
				continue
			}

			// Extract content delta
			if len(streamResp.Choices) > 0 && streamResp.Choices[0].Message.Content != "" {
				select {
				case tokens <- streamResp.Choices[0].Message.Content:
				case <-ctx.Done():
					return
				}
			}
		}
	}()

	return tokens, nil
}

// ModelName returns the model name.
func (p *TemplateProvider) ModelName() string {
	return p.GetModel("")
}

// MaxTokens returns the max tokens setting.
func (p *TemplateProvider) MaxTokens() int {
	return p.GetMaxTokens(0)
}

// ============================================================================
// STEP 6: Implement helper methods
// ============================================================================

// execute performs a single HTTP request.
func (p *TemplateProvider) execute(ctx context.Context, req *TemplateRequest) (*TemplateResponse, error) {
	model := p.GetModel("")

	resp, err := p.client.R().
		SetContext(ctx).
		SetBody(req).
		Post(p.baseURL + "/chat/completions")

	if err != nil {
		return nil, agentErrors.NewLLMRequestError(p.ProviderName(), model, err)
	}

	if !resp.IsSuccess() {
		return nil, p.handleHTTPError(resp, model)
	}

	var templateResp TemplateResponse
	if err := json.NewDecoder(strings.NewReader(resp.String())).Decode(&templateResp); err != nil {
		return nil, agentErrors.NewLLMResponseError(p.ProviderName(), model, constants.ErrFailedDecodeResponse)
	}

	return &templateResp, nil
}

// executeWithRetry executes request with exponential backoff using shared retry logic.
func (p *TemplateProvider) executeWithRetry(ctx context.Context, req *TemplateRequest) (*TemplateResponse, error) {
	return ExecuteWithRetry(ctx, DefaultRetryConfig(), p.ProviderName(), func(ctx context.Context) (*TemplateResponse, error) {
		return p.execute(ctx, req)
	})
}

// handleHTTPError maps HTTP errors to AgentError using the shared helper.
func (p *TemplateProvider) handleHTTPError(resp *resty.Response, model string) error {
	httpErr := RestyResponseToHTTPError(resp)
	return MapHTTPError(httpErr, p.ProviderName(), model, func(body string) string {
		var errResp TemplateErrorResponse
		if err := json.Unmarshal([]byte(body), &errResp); err == nil && errResp.Error.Message != "" {
			return errResp.Error.Message
		}
		return body
	})
}

// convertResponse converts provider response to standard response.
func (p *TemplateProvider) convertResponse(resp *TemplateResponse) *agentllm.CompletionResponse {
	if len(resp.Choices) == 0 {
		return &agentllm.CompletionResponse{
			Model:    resp.Model,
			Provider: p.ProviderName(),
		}
	}

	return &agentllm.CompletionResponse{
		Content:      resp.Choices[0].Message.Content,
		Model:        resp.Model,
		TokensUsed:   resp.Usage.TotalTokens,
		FinishReason: resp.Choices[0].FinishReason,
		Provider:     p.ProviderName(),
		Usage: &interfaces.TokenUsage{
			PromptTokens:     resp.Usage.PromptTokens,
			CompletionTokens: resp.Usage.CompletionTokens,
			TotalTokens:      resp.Usage.TotalTokens,
		},
	}
}

// ============================================================================
// STEP 7: (Optional) Implement tool calling support
// ============================================================================

// Uncomment and implement if your provider supports tool calling:
//
// func (p *TemplateProvider) GenerateWithTools(ctx context.Context, prompt string, tools []interfaces.Tool) (*ToolCallResponse, error) {
//     // Convert tools to provider format
//     // Make API call
//     // Parse response and extract tool calls
//     return nil, fmt.Errorf("not implemented")
// }
//
// func (p *TemplateProvider) StreamWithTools(ctx context.Context, prompt string, tools []interfaces.Tool) (<-chan ToolChunk, error) {
//     return nil, fmt.Errorf("not implemented")
// }

// ============================================================================
// STEP 8: (Optional) Implement embedding support
// ============================================================================

// Uncomment and implement if your provider supports embeddings:
//
// func (p *TemplateProvider) Embed(ctx context.Context, text string) ([]float64, error) {
//     // Make embedding API call
//     // Return embedding vector
//     return nil, fmt.Errorf("not implemented")
// }
